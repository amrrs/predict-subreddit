{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnei50-DbgjG"
      },
      "outputs": [],
      "source": [
        "!pip install praw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQU7GGXV_Td_"
      },
      "source": [
        "### Scrapping top subreddits from redditlist.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf0Gx7f1nG94"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "npage=2\n",
        "subreddits = []\n",
        "for n in range(1, npage+1):\n",
        "    r = requests.get(f\"http://www.redditlist.com/?page={n}\")\n",
        "    soup = BeautifulSoup(r.text, 'lxml')\n",
        "    subreddits.extend([x.text for x in soup.find(id=\"listing-parent\").findAll(class_=\"span4 listing\")[1].findAll(\"a\", attrs={\"class\": \"sfw\"})])\n",
        "subreddits.remove(\"announcements\") # r/announcements is for company statements\n",
        "subreddits.remove(\"wallstreetbets\") # can't scrape r/wallstreetbets\n",
        "len(subreddits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNYSGiIk_kll"
      },
      "source": [
        "### Getting top posts titles from subreddits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu1fqJcFF0pU"
      },
      "outputs": [],
      "source": [
        "import praw\n",
        "import re\n",
        "\n",
        "# pass your authentication information to Reddit instance\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"\",\n",
        "    client_secret=\"\",\n",
        "    user_agent=\"\",\n",
        "    check_for_async=False,\n",
        ")\n",
        "\n",
        "rows=[]\n",
        "for subreddit_name in subreddits:\n",
        "    print(f'scraping {subreddit_name}')\n",
        "    subreddit = reddit.subreddit(subreddit_name) \n",
        "\n",
        "    for post in subreddit.top(limit=None):\n",
        "        title = post.title\n",
        "        title = re.sub(\"\\[.*?\\]\",'',title) # removing flair information from title\n",
        "        title = '\"'+title+'\"' # wrapping inside double quotes to deal with commas in the title text\n",
        "        rows.append([title, subreddit_name])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-U9iNxAABPt"
      },
      "source": [
        "### Writing the tiles to csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0mJw6v-J7Cc"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "with open('posts.csv', 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"title\", \"label\"]) # headers\n",
        "    writer.writerows(rows)\n",
        "\n",
        "print(\"All Done!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}